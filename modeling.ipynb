{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import wandb\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from models.unet import *\n",
    "from models.simple_model import *\n",
    "from utils.data_utils.acdc_datamodule import *\n",
    "from utils.data_utils.data_utils import *\n",
    "from utils.model_utils.dice_score import *\n",
    "from utils.model_utils.resnet_loss import ResnetLoss \n",
    "\n",
    "from lightning.pytorch.callbacks import RichProgressBar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSegmanter(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate, criterion) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        model_output = self.model(x)\n",
    "        return model_output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, ground_truths = batch\n",
    "        masks_pred = self.model(images)\n",
    "        loss = self.criterion(masks_pred, ground_truths)\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, ground_truths = batch\n",
    "        masks_pred = self.model(images)\n",
    "        # ground_truths = ground_truths.long()\n",
    "        loss = self.criterion(masks_pred, ground_truths)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, ground_truths = batch\n",
    "        masks_pred = self.model(images)\n",
    "        ground_truths = ground_truths.long()\n",
    "        loss = self.criterion(masks_pred, ground_truths)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constans and Hyperparams\n",
    "NUM_CLASSES = 4\n",
    "MAX_EPOCHS = 500\n",
    "\n",
    "# Big model takes lots of space in memory -> small batch size fits in\n",
    "BATCH_SIZE_TRAIN = 8\n",
    "BATCH_SIZE_VAL = 8\n",
    "BATCH_SIZE_TEST  = 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = DualTransform(20,0.2,0.2)\n",
    "datamodule = ACDCDataModule(\"database\", BATCH_SIZE_TRAIN,BATCH_SIZE_VAL,BATCH_SIZE_TEST,(256,256,1), convert_to_single=False, transform=transform)\n",
    "datamodule.setup(\"fit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datamodule.acdc_val.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n",
    "\n",
    "\n",
    "wandb_logger = pl.loggers.WandbLogger(project=\"Medical Image Segmentation\")\n",
    "\n",
    "model = smp.Unet('resnet34',classes=NUM_CLASSES, in_channels=1)\n",
    "\n",
    "# model = fcn_resnet50()\n",
    "# model.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# model.classifier[4] = nn.Conv2d(512, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "# model = UNet(n_channels=1, n_classes=4)\n",
    "\n",
    "# model = SimpleSegmentationModel(1,4)\n",
    "\n",
    "wandb_logger.watch(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diceloss kiegyensúlyozatlan osztályokra\n",
    "# CrossEntropy is jó lehet, de Dice is jó \n",
    "# Jacquard index - IoU\n",
    "# ignore index kipróbálása -> metrikánál mindenképp érdemes\n",
    "#  1 ignorált klassznak lehet nincs hatása\n",
    "#  ignorált osztály -> lehet crossentropy is azonos hatékonyságú\n",
    "#  osztályonkénti dice coeff\n",
    "# split betegenként, fixáljuk a validációs halmazt\n",
    "#  KFold ha nem fixálunk -> ez is lehet ensemble\n",
    "# pixelszintű tévesztési mátrix, pixel accuracy \n",
    "#  legyenek példapredikciók\n",
    "#  kvalitatív kiértékelés\n",
    "\n",
    "\n",
    "criterion = smp.losses.DiceLoss(mode=\"multiclass\", ignore_index=0)\n",
    "# loss = ResnetLoss(criterion)\n",
    "loss = criterion\n",
    "# loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import Callback\n",
    "\n",
    "class_labels = {\n",
    "  0: \"background\",\n",
    "  1: \"RV\",\n",
    "  2: \"myocardium\",\n",
    "  3: \"LV\"\n",
    "}\n",
    "\n",
    "class Visualizer(Callback):\n",
    "    def on_validation_epoch_start(self, trainer, model):\n",
    "        x = trainer.datamodule.acdc_val[0][0]\n",
    "        y = trainer.datamodule.acdc_val[0][1]\n",
    "        pred = model(x.unsqueeze(0).to('cuda'))\n",
    "        gt_mask = np.array(y.squeeze())\n",
    "        pred_mask = np.array(torch.argmax(pred.squeeze().cpu(), dim=0))\n",
    "        error = (gt_mask == pred_mask).astype(np.uint8)\n",
    "        trainer.logger.experiment.log(\n",
    "            {\"visualizing\":[\n",
    "                    wandb.Image(x, caption=\"GT\", masks={\n",
    "                        \"segmentation\": {\n",
    "                            \"mask_data\": gt_mask,\n",
    "                            \"class_labels\": class_labels\n",
    "                        },\n",
    "                    }),\n",
    "                    wandb.Image(x, caption=\"pred\", masks={\n",
    "                        \"segmentation\": {\n",
    "                            \"mask_data\": pred_mask,\n",
    "                            \"class_labels\": class_labels\n",
    "                        },\n",
    "                    }),\n",
    "                    wandb.Image(error, caption=\"error\"),\n",
    "                ]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May need to add new preprocessing arg, to include pretrained model preprocessing\n",
    "# preprocess_input = smp.encoders.get_preprocessing_fn('resnet18', pretrained='imagenet')\n",
    "\n",
    "\n",
    "\n",
    "segmenter = SemanticSegmanter(model = model, learning_rate=1e-4 ,criterion=loss)\n",
    "\n",
    "# tsmp.metrics.functional.IoU or torch metric?\n",
    "# do we need this?\n",
    "# metric = smp.metrics.functional.IoU(threshold=0.5)\n",
    "\n",
    "\n",
    "# Configure callbacks and logger\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',  patience=5 ,mode=\"min\", verbose=True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,  \n",
    "    logger=wandb_logger, \n",
    "    callbacks = [\n",
    "        RichProgressBar(),\n",
    "        Visualizer(),\n",
    "        early_stopping\n",
    "        ]\n",
    "    )\n",
    "trainer.fit(segmenter, datamodule=datamodule)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
