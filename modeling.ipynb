{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import wandb\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from models.unet import *\n",
    "from models.simple_model import *\n",
    "from models.tri_unet import *\n",
    "from models.divergent_nets import *\n",
    "from utils.data_utils.acdc_datamodule import *\n",
    "from utils.data_utils.data_utils import *\n",
    "from utils.model_utils.dice_score import *\n",
    "from utils.model_utils.resnet_loss import ResnetLoss \n",
    "\n",
    "from lightning.pytorch.callbacks import RichProgressBar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbanfizsombor1999\u001b[0m (\u001b[33mdrigba\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSegmanter(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate, criterion) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        model_output = self.model(x)\n",
    "        return model_output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, ground_truths = batch\n",
    "        masks_pred = self.model(images)\n",
    "        ground_truths = ground_truths.long()\n",
    "        loss = self.criterion(masks_pred, ground_truths)\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, ground_truths = batch\n",
    "        masks_pred = self.model(images)\n",
    "        ground_truths = ground_truths.long()\n",
    "        loss = self.criterion(masks_pred, ground_truths)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred_y = self(x)\n",
    "\n",
    "        # train dice\n",
    "        y_pred = torch.argmax(pred_y[2], axis=1)\n",
    "        y_pred_onehot = F.one_hot(y_pred, 4).permute(0, 3, 1, 2)\n",
    "        dice = self.compute_dice(y_pred_onehot, y)\n",
    "        dice_LV = dice[3]\n",
    "        dice_RV = dice[1]\n",
    "        dice_MYO = dice[2]\n",
    "        self.log('dice/all_train_dice', dice[1:].mean(), on_epoch=True, prog_bar=True, logger=True, on_step=False)\n",
    "        self.log('dice/train_LV_dice', dice_LV, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n",
    "        self.log('dice/train_RV_dice', dice_RV, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n",
    "        self.log('dice/train_MYO_dice', dice_MYO, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n",
    "        # save grad\n",
    "        for name, params in self.named_parameters():\n",
    "            if params.grad is not None:\n",
    "                self.log(f'abs_{name}',params.grad.abs().mean(), on_epoch=True)\n",
    "        return loss #TODO\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constans and Hyperparams\n",
    "NUM_CLASSES = 4\n",
    "MAX_EPOCHS = 500\n",
    "\n",
    "# Big model takes lots of space in memory -> small batch size fits in\n",
    "BATCH_SIZE_TRAIN = 8\n",
    "BATCH_SIZE_VAL = 8\n",
    "BATCH_SIZE_TEST  = 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = DualTransform(20,0.2,0.2)\n",
    "datamodule = ACDCDataModule(\"database\", BATCH_SIZE_TRAIN,BATCH_SIZE_VAL,BATCH_SIZE_TEST,(256,256,1), convert_to_single=False, transform=transform)\n",
    "datamodule.setup(\"fit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterTriUnet(nn.Module):\n",
    "    def __init__(self, feature_model, pred_model, n_heads =2):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.feature_model = feature_model\n",
    "        self.pred_model = pred_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.repeat(1, self.n_heads, 1, 1)\n",
    "        output = self.feature_model(x)\n",
    "        return self.pred_model(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n",
    "\n",
    "\n",
    "wandb_logger = pl.loggers.WandbLogger(project=\"Medical Image Segmentation\")\n",
    "\n",
    "# Unet\n",
    "model = smp.Unet('resnet34',classes=NUM_CLASSES, in_channels=1)\n",
    "\n",
    "# TriUnet\n",
    "# feature_models = [smp.Unet('resnet34',classes=NUM_CLASSES, in_channels=1) for _ in range(2)]\n",
    "# feature_models = torch.nn.ModuleList(feature_models)\n",
    "# model = TriUnet(feature_models, smp.Unet('resnet34',classes=NUM_CLASSES, in_channels=len(feature_models)*NUM_CLASSES))\n",
    "\n",
    "# Resnet\n",
    "# model = fcn_resnet50()\n",
    "# model.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# model.classifier[4] = nn.Conv2d(512, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "# Unet local\n",
    "# model = UNet(n_channels=1, n_classes=4)\n",
    "\n",
    "# Simple model\n",
    "# model = SimpleSegmentationModel(1,4)\n",
    "\n",
    "# Simple model + triunet\n",
    "# feature_models = [smp.Unet('resnet34',classes=NUM_CLASSES, in_channels=1) for _ in range(2)]\n",
    "# # feature_models = [SimpleSegmentationModel(in_channels=1, out_channels=NUM_CLASSES) for _ in range(2)]\n",
    "# feature_models = torch.nn.ModuleList(feature_models)\n",
    "# model = TriUnet(feature_models, SimpleSegmentationModel(in_channels=len(feature_models)*NUM_CLASSES, out_channels=NUM_CLASSES))\n",
    "# model = BetterTriUnet(UNet(1,4, n_heads=2), UNet(2*4,4, n_heads=1))\n",
    "# model = UNet(1,4)\n",
    "wandb_logger.watch(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "model1 = smp.Unet('resnet34',classes=NUM_CLASSES, in_channels=1).cuda()\n",
    "model2 = UNet(n_channels=1, n_classes=4).cuda()\n",
    "model3 = fcn_resnet50()\n",
    "model3.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model3.classifier[4] = nn.Conv2d(512, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n",
    "model3 = model3.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1.7235326766967773 seconds\n",
      "Time taken: 14.67308259010315 seconds\n",
      "Time taken: 6.882024526596069 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "criterion = smp.losses.DiceLoss(mode=\"multiclass\")\n",
    "for ix, m in enumerate([model1, model2, model3]):\n",
    "    m.train()\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "    for x,y in datamodule.train_dataloader():\n",
    "        out = m(x.to(\"cuda\"))\n",
    "        if ix == 2:\n",
    "            loss = ResnetLoss(criterion)\n",
    "            loss(out, y.to(\"cuda\").long()).backward()\n",
    "        else:\n",
    "            criterion(out, y.to(\"cuda\").long()).backward()\n",
    "        i+=1\n",
    "        if i>3:\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Time taken: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diceloss kiegyensúlyozatlan osztályokra\n",
    "# CrossEntropy is jó lehet, de Dice is jó \n",
    "# Jacquard index - IoU\n",
    "# ignore index kipróbálása -> metrikánál mindenképp érdemes\n",
    "#  1 ignorált klassznak lehet nincs hatása\n",
    "#  ignorált osztály -> lehet crossentropy is azonos hatékonyságú\n",
    "#  osztályonkénti dice coeff\n",
    "# split betegenként, fixáljuk a validációs halmazt\n",
    "#  KFold ha nem fixálunk -> ez is lehet ensemble\n",
    "# pixelszintű tévesztési mátrix, pixel accuracy \n",
    "#  legyenek példapredikciók\n",
    "#  kvalitatív kiértékelés\n",
    "\n",
    "\n",
    "criterion = smp.losses.DiceLoss(mode=\"multiclass\")\n",
    "loss = ResnetLoss(criterion)\n",
    "loss = criterion\n",
    "# loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import Callback\n",
    "\n",
    "class_labels = {\n",
    "  0: \"background\",\n",
    "  1: \"RV\",\n",
    "  2: \"myocardium\",\n",
    "  3: \"LV\"\n",
    "}\n",
    "\n",
    "class Visualizer(Callback):\n",
    "    def on_validation_epoch_start(self, trainer, model):\n",
    "        x = trainer.datamodule.acdc_val[0][0]\n",
    "        y = trainer.datamodule.acdc_val[0][1]\n",
    "        pred = model(x.unsqueeze(0).to('cuda'))\n",
    "        gt_mask = np.array(y.squeeze())\n",
    "        pred_mask = np.array(torch.argmax(pred.squeeze().cpu(), dim=0))\n",
    "        error = (gt_mask == pred_mask).astype(np.uint8)\n",
    "        trainer.logger.experiment.log(\n",
    "            {\"visualizing\":[\n",
    "                    wandb.Image(x, caption=\"GT\", masks={\n",
    "                        \"segmentation\": {\n",
    "                            \"mask_data\": gt_mask,\n",
    "                            \"class_labels\": class_labels\n",
    "                        },\n",
    "                    }),\n",
    "                    wandb.Image(x, caption=\"pred\", masks={\n",
    "                        \"segmentation\": {\n",
    "                            \"mask_data\": pred_mask,\n",
    "                            \"class_labels\": class_labels\n",
    "                        },\n",
    "                    }),\n",
    "                    wandb.Image(error, caption=\"error\"),\n",
    "                ]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_dice(self, pred_y, y):\n",
    "        \"\"\"\n",
    "        Computes the Dice coefficient for each class in the ACDC dataset.\n",
    "        Assumes binary masks with shape (num_masks, num_classes, height, width).\n",
    "        \"\"\"\n",
    "        epsilon = 1e-6\n",
    "        num_masks = pred_y.shape[0]\n",
    "        num_classes = pred_y.shape[1]\n",
    "        dice_scores = torch.zeros((num_classes,), device=self.device)\n",
    "\n",
    "        for c in range(num_classes):\n",
    "            intersection = torch.sum(pred_y[:, c] * y[:, c])\n",
    "            sum_masks = torch.sum(pred_y[:, c]) + torch.sum(y[:, c])\n",
    "            dice_scores[c] = (2. * intersection + epsilon) / (sum_masks + epsilon)\n",
    "        print(dice_scores)\n",
    "        return dice_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation(Callback):\n",
    "    def on_test_epoch_start(self, trainer, model):\n",
    "        x = trainer.datamodule.acdc_val[0][0]\n",
    "        y = trainer.datamodule.acdc_val[0][1]\n",
    "        pred = model(x.unsqueeze(0).to('cuda'))\n",
    "        gt_mask = np.array(y.squeeze())\n",
    "        pred_mask = np.array(torch.argmax(pred.squeeze().cpu(), dim=0))\n",
    "        y_pred_onehot = F.one_hot(pred_mask, 4).permute(0, 3, 1, 2)\n",
    "        dice = compute_dice(y_pred_onehot, gt_mask)\n",
    "        dice_LV = dice[3]\n",
    "        dice_RV = dice[1]\n",
    "        dice_MYO = dice[2]\n",
    "        self.log('dice/all_train_dice', dice[1:].mean(), on_epoch=True)\n",
    "        self.log('dice/train_LV_dice', dice_LV, on_epoch=True)\n",
    "        self.log('dice/train_RV_dice', dice_RV, on_epoch=True)\n",
    "        self.log('dice/train_MYO_dice', dice_MYO, on_epoch=True)\n",
    "        # save grad\n",
    "        for name, params in self.named_parameters():\n",
    "            if params.grad is not None:\n",
    "                self.log(f'abs_{name}',params.grad.abs().mean(), on_epoch=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May need to add new preprocessing arg, to include pretrained model preprocessing\n",
    "# preprocess_input = smp.encoders.get_preprocessing_fn('resnet18', pretrained='imagenet')\n",
    "\n",
    "\n",
    "\n",
    "segmenter = SemanticSegmanter(model = model, learning_rate=1e-4 ,criterion=loss)\n",
    "\n",
    "# tsmp.metrics.functional.IoU or torch metric?\n",
    "# do we need this?\n",
    "# metric = smp.metrics.functional.IoU(threshold=0.5)\n",
    "\n",
    "\n",
    "# Configure callbacks and logger\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',  patience=5 ,mode=\"min\", verbose=True)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,  \n",
    "    logger=wandb_logger, \n",
    "    callbacks = [\n",
    "        RichProgressBar(),\n",
    "        Visualizer(),\n",
    "        early_stopping\n",
    "        ]\n",
    "    )\n",
    "trainer.fit(segmenter, datamodule=datamodule)\n",
    "# trainer.test(segmenter, datamodule)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
